# Linux 音频系统简析
我之所以钻研linux内核，只想是知道为什么我的系统还是没有声音……

linux音频系统架构问题由来已久……远远比你想像的复杂。如果你想理清从读取音频文件到最终从你的扬声器中播放出来这一过程中所用到的技术之间的关系的话，纸上的结构图足以像炸酱面一样混乱，而你根本找不到任何头绪。

这是因为，音频系统本身就比其他架构更加复杂。OSI模型每一层都有自己的作用域和功能，每一层几乎不会有任何交集，所以你绝对不会碰到任何混乱情况。但是，在linux音频系统上，却上演着这样的事情：没有明确的底层，各种音频技术各自为政。linux的音频系统架构有点像地壳构造，偶尔就地震一下，要不就火山爆发一下，上层结构则要使劲遮掩发生的一切。

Open Sound Protocol（开放声音协议？），原本用来内核和声卡通信的（驱动），但是现在却成了alsa的一个胶合层（也不错，驱动声卡）。alsa兼顾底层和硬件通信，为应用程序提供api，即负责混音，又负责硬件资源调用，多声道支持，环绕立体声输出等，甚至还要负责mp3解码（什么时候alsa负责mp3解码了？我out了）。所以当众多发行版使用PulseAudio或者Gstreamer时候，错误就不断的发生了…

ALSA


 

输入: PulseAudio, Jack, GStreamer, Xine, SDL, ESD

输出: Hardware, OSS

首先，让我们从了解alsa（Advanced Linux Sound Architecture, 高级linux音频架构）开始。alsa直接和内核通信，并提供音频接口功能以供调用。但是，似乎alsa做了“比当好一个驱动程序”更多的事情：为系统混音，为其他程序提供音频输出输出接口，为程序员提供api。他的目标好像要同Windows的ASIO或者OS X的CoreAudio一样，作为一个底层而稳定的后台程序运行。

本来alsa是设计成为oss的继任者的，值得庆幸的是，oss并没有真的死亡，凭借着alsa的兼容层重生了。所以可以简单的把alsa理解为声卡的驱动层。实际驱动声卡的还是oss。声卡需要加载前缀为snd_的内核驱动模块，以在发声事件时驱动声卡发声。这也就是你需要linux声卡驱动的原因，这也可能是你笔记本不出声音的原因…

幸运的是，大部分的发行版都已经自动配置好相关设备以及驱动模块，alsa负责提供api给应用程序，应用程序可以调用api发声。最初这个设计是给oss用的～（当时大部分驱动都是如此），但是会引发声卡独占问题（即只有一个程序可以发声，其他程序只能进入队列等待）。

alsa需要一个软件部件监测声卡并管理声卡。当有两个或多个程序需要同时发声，alsa则进行软混音——如果你的声卡支持，则使用声卡硬混音。alsa最多可以同时管理8路的声频硬件，还可以同时支持mid特性。当然这个特性要取决于你的计算机硬件，所以随着硬件的发展，这个特性不是显得那么重要了。

alsa和其他驱动不同之处是它的可制定性。也正是因为高度的可制定性，导致linux音频系统架构越来越复杂。通过配置文件（/usr/share/alsa/alsa.conf）你可以管理一切——不论是混音方式，输出设备选择，采样率，比特深度，还是实时音效。

alsa因其透明性、高效性和灵活性使之成为 了Linux音频系统的标准，也成为了几乎其他所有的音频架构和硬件通信的桥梁。

PulseAudio
输入: GStreamer, Xine, ALSA

输出: ALSA, Jack, ESD, OSS

如果你认为让alsa当后台就万无一失，那可就大错特错了。alsa虽然可以管理硬件，但是软件层却是其力所不及的地方。



这就是pulse，连接软件和硬件，远程计算机和本地计算机。它可以像alsa那样处理本地音频流，可以更灵活的处理远程计算机的音频流并在本地发声。因为其灵活性，可用性，已经被众多linux发行版所采用（为什么arch还是alsa而不是pulse，囧）。

附加效果和alsa一样，高度的灵活性带来了高度的复杂性。但是似乎pulse的问题更为复杂，因为pulse是面向用户设计的，所以用户的错误配置可能轻易的引起故障。所以就算是ubuntu，系统也尽量不会让用户更改其配置文件。

当你使用面板上的音量调节工具调节音量时，实际上你调节的是个虚拟设备——你调节pulse的虚拟设备，pulse调节alsa，alsa反馈给pulse，pulse再反馈给虚拟设备……（多纠结啊）

好像pulse没有给linux音频系统带来什么增益？所以反对的声音不绝于耳（他们觉得和alsa相比就是重复造轮子？）。他没有使已有的操作起来更简单（指alsa），也没有带来更好地音效（本子想要hifi效果？），但是它带来了几个非常重要的特性。首当其冲就是混音特性。

如果所有的程序都使用pulse，一切都将变得美好起来。开发者们不用再考虑系统复杂性，因为pulse是跨平台的。但正是因为如此，所以有这么多其他的音频解决方案的主要原因之一。

不像alsa那样，pulse可以跨平台，运行在不同的操作系统上，包括POSIX标准的unix-like系统和微软的Windows。也就是说呢，如果你写的程序使用pulse而不是alsa，那么移植起来会非常轻松。

事情其实并不简单，因为在linux上，pulse依赖于alsa，pulse把自身模拟成输出设备，供alsa调用。这点有点和Jack相似，处在alsa和桌面之间，使用管道来传输数据。

不同于jack，它不主动添加或删除音频源，所以你可以对所有输出的音频程序音量等进行分控，哈哈，至少用这个特性你可以让那些吵人的网站全部静音（firefox静音吗？flashblock就好了嘛～）



GStreamer
输入: Phonon

输出: ALSA, PulseAudio, Jack, ESD

算上gstreamer，linux的音频系统更加复杂了……因为gstreamer有点像pulse，没有什么新的特性。他是诞生在pulse之前，而且拥有更多的开发者。看起来更象是gnome专属的一个多媒体框架。它是为数不多的可以安装并使用解码器的架构之一。他也是GTK开发者的首选，使用gstreamer你甚至可以为Palm Pre进行程序开发。

gstreamer介于软件层和音频输入层之间，优先于pulseaudio。gstreamer与众不同之处在于他不只是个音频处理框架，通过安装解码器，你还可以通过他来播放音频视频文件。

例如播放mp3，通常是下载相应的gstreamer解码器并安装。linux下唯一一个官方授权的商业版的Fluendo MP3解码器，是一个gstreamer插件，其实mpeg，h264等也都是如此。

Jack
输入: PulseAudio, GStreamer, ALSA,

输出: OSS, FFADO, ALSA

拥有pulse开放性等特点，通过管道传输音频流，最终使用输出设备输出音频。jack是个中间层，音频和程序的远程进程信号等同，这使得应用程序可以通过组件建立。



最好的例子就是虚拟录音，应用程序在录音的同时可以对音频进行处理，并把处理好的数据通过一个虚拟设备“输出”到应用程序。实际录音情况中有可能使通过网络或者电缆等，jack同样对输入的音频流进行处理。

jack是jack音频连接工具包的简称，由于其低延时，面向底层设计，所以不会发生因为需要处理过多数据而缓慢的情况。但是如果需要使用jack，音频程序需要进行相应的编码，专为jack而设计。jack不像alsa，pulse那样简单，他需要运行在系统的最高优先级，而且需要专门的设备输入。

在jack兼容的程序中，你自由的选择音频输入方式，例如你可以直接使用audacity录制当前vlc输出的音频。或者你可以通过JackRack，建立包括ping延迟、多道混响和语言编码等多种实时效果的应用程序来发送它。

jack对于多媒体工作站是不二之选。Ardour就是使用jack作为输入输出组件。jack是如此之出色，甚至你在mac上也能看到他的身影。

FFADO
输入: Jack

输出: Audio hardware

许多专业设备都是通过“火线”连接到pc的。这样做有很多优点，火线传输速率快，而且可以为外设供电。很多台式机和笔记本都有火线插口，它是如此稳定而成熟。在外面，你可以通过笔记本的火线插口录制音频，回到工作室再导入工作站中。

与usb不同，它是专用于音频输入输出的插口，拥有自己的通信协议，无须安装驱动程序。如此的复杂性，导致alsa不能胜任。所以它使用专属于自己的胶合层。

起初，这是个叫FreeBOB的项目，这得益于许多火线音频设备都是基于相同的硬件协议。FFADO是FreeBOB的继任者，提供更多特性，并为其他类型的火线插口设备提供支持。

2009年末，他发布了第二版，包含了对许多类似Alesis、Apogee、 ART、CME、Echo、Edirol、Focusrite、M-Audio、Mackie、Phonic和Terratec的单元的支持。因为不能确保所有的硬件都能在这个平台上工作，所以购买前你需要进行测试（livecd测试ati显卡？lol），很多厂商也提供FFADO开发者设备以支持其改进和驱动完善。

FFADO另一个特性是整合了dsp芯片的混音驱动，你可以通过图形界面设置输入输出，以及音效等。不同于alsa的软混音，你可以真正的对硬件进行控制，做到真正的0延时，这对现场录音等需求大大有助。

和alsa等其他架构不同，jack仅仅对其支持的硬件进行处理，没有对alsa或者pulse提供接口，除非你用alsa替代jack，否则你无法使用jack正常的进行音频播放。但是很多专业设备对jack支持良好，所以jack是你的最优选择。

Xine
输入: Phonon

输出: PulseAudio, ALSA, ESD

如果说linux音频发展像地球史，那xine就处在白垩纪。它就像个遗老，你仍能从很多播放器中发现它的身影，所以很多linux发行版仍然捆绑着xine。

xine创立之初，设计分为前端和后端，前端用于和用户交互，后端处理多媒体。得益于封装的解码库，它可以播放包括AVI、Matroska和Ogg以及它们 包含的数十种格式，例如AAC、Flack、MP3、Vorbis和WMA。

因为它依赖于库实现，所以xine被开发成一个多媒体框架，库的开发，使得xine在法律允许范围内对多媒体文件提供最好的支持。xine可以和alsa，pulse通信，很多程序也可以调用xine，例如totme-xine。同时xine也是kde的Phonon默认后端，所以不论是Amarok 还是 Kaffeine，都能看到他的踪迹。

Phonon
输入: Qt 和 KDE 程序

输出: GStreamer, Xine

跨平台是他最大的优势。开发者可以用Qt在Linux上编写一个音乐播放器，然后可以重编译给OS X和Windows使用，而无需要考虑音乐是如何播放的、使用的音频硬件的兼容性如何，或者最终操作系统会如何处理音频这些问题。Qt的Phonon自动完成音频传送，例如自动传送到OS X的CoreAudio的API中，或者Windows的DirectSound中。
在Linux平台上（不同于早先的KDE版本），得益于其透明的编解码器的支持，Phonon传递音频给GStreamer。phonon正在慢慢从Qt的框架中剥离。
这个架构受到最大的批评就是它过于简单，没有任何新的特性，不过看样子KDE 4中，还是会将它保留在架构中的。
其他分支
当然还有其他很多小众的音频技术，例如ESD、SDL和 PortAudio。

ESD是声音启发守护进程（Enlightenment Sound Daemon），它在曾经很长的一段时间里曾是Gnome桌面的默认声音服务。后来，Gnome开始使用libcanberra（它本身可以和ALSA、 GStreamer、OSS和PulseAudio通信），ESD在2009年4月被彻底放弃支持。在kde上esd也是杯具。因为大部分人都是使用kde4，所以phonon替代了esd。慢慢的esd坠入历史长河…

SDL依然欣欣向荣的发展着。因为已经是用他开发了上百款跨平台游戏， 所以SDL库的音频输出组件依然支持良好，具有大量新的特性，并且成熟而稳定。

PortAudio也是一个跨平台音频库，它把 SGI、Unix和Beos加入到可能的终端混音器中。使用PortAudio的最知名的应用程序就是Audacity音频编辑器了，因为使用了portaudio，使得它音频输出遇到了问题，jack支持也遇到了bug（audacity得不偿失啊）

OSS，开放声音系统（Open Sound System）。它已经被从linux 2.4之后的版本中剥离，但是他仍然存在……主要原因是有太多的程序仍然在使用它。不像 alsa，它可以在除Linux的平台上运行。它甚至还有个FreeBSD版本……（老当益壮～）在1992年，它可以说是一个很好的系统，但是现在它几乎被alsa替换。OSS定义了Linux下音频的工作方式，特别是音频设备要通过ioctl分支访问，例如通过/dev/dsp。ALSA提供了一个OSS兼容层来让那些使用OSS的程序仍旧可以使用alsa标准。OSS项目曾经作过开源和专利开发的尝试，现在仍作在为4项前端技术（4 Front Technologies）的商业开发而努力，并在2009年11月发布了OSS 4.2的2002版。

GStreamer  
GStreamer LogoGStreamer是一个Linux桌面媒体流系统的事实标准。它支持音视频流的编解码。可以基于该API实现从简单的音频文件播放到网络流媒体配置等大范围的应用。GStreamer支持多种编解码器和音频后端。GStreamer显然不适用于基本的PCM音频播放或者低延时/实时应用场合。GStreamer是可移植的，并不限于仅能在Linux上使用。它支持的音频后端包括ALSA、OSS、PulseAudio等。
[APIReference]
libcanberra   
GNomelibcanberra是一个抽象的事件声音API。它是XDG声音主题和命名规范的实现。虽然libcanberra是GNOME的一部分，但它并不依赖于GNOME/Gtk/GLib，因此可以用于其它桌面环境。它除了提供简单的界面用于播放声音文件之外，还提供了缓冲机制（适用于基于网络的瘦客户机）和允许传递多种元数据到底层的声音系统进行控制从而增强用户体验（比如定位事件声音）和改善易操作性。libcanberra支持多种后端，并且是可移植的。目前支持的后端包括ALSA、OSS、PulseAudio、GStreamer等。
[APIReference]
JACK  
JACK LogoJACK是一个用于连接专业音频应用程序和硬件输入输出的声音系统。该系统专注于低延时和应用程序之间的互联接，因此，该系统对于普通的桌面应用或者嵌入式应用都不适合。如果你想做的只是简单的PCM音频播放，这个系统也不是很有用。JACK支持多种后端，其中支持得最好的是ALSA。JACK也是可移植的。
[APIReference]
Full ALSA  
ALSA LogoALSA是专门面向PCM音频播放和录音的LinuxAPI。ALSA专注于硬件设备，但也支持其他一些后端。ALSA这个名字既表示了Linux内核的音频驱动程序，也表示了围绕着驱动程序而建立的用户空间函数库。ALSA 函数库是全面且可移植的。完整的ALSA API可以是非常复杂和巨大的，但它能几乎能支持所有的音频设备。不过ALSA API的某些功能实际取决于Linux核心和对应的Linux驱动程序对硬件的支持。比如基于软件的声音服务器或者在用户空间实现的音频驱动程序（蓝牙和火线音频支持）。
[APIReference]
Safe ALSA  
ALSA Logo在全部的ALSAAPI中，只有一个子集是可以在所有ALSA支持的后端上工作的。因此，如果编写使用了ALSA的软件，并且需要软件是可移植、向后兼容和与各种音频服务器、蓝牙音频和火线音频都兼容的话，你应该只使用这个被称为SafeALSA的安全API子集。该子集是基础的、可移植的PCM音频播放和录音功能的实现，而且除了支持ALSA核心本身所支持的设备之外，还支持OSS、PulseAudio、JACK等。
Phonon 和 KNotify  
PhononPhonon是高层次的媒体流系统的抽象实现，比如GStreamer便是一种。但Phonon比GStreamer更深入。KNotify是一个用于“提示”的系统，支持的不仅仅是事件声音。但目前还不支持 XDG Sound Theming/NamingSpecifications。而且并不支持缓冲或传递事件元数据到更下层的音频系统。KNotify支持多种音频后端来通过Phonon进行播放。这两者的API都是仅面向KDE/Qt体系的，所以不能在该体系之外的应用程序中使用。
[PhononAPIReference] [KNotifyAPIReference]
SDL  
SDL LogoSDL是一种跨平台、可移植的API，主要用于全屏幕的游戏开发。SDL包括很多东西，其中的一部分是一个可移植的音频界面。SDL也支持OSS、PulseAudio和ALSA作为后端。
[APIReference]
PulseAudio  
Pulse Audio LogoPulseAudio是一种适用于Linux桌面和嵌入式环境的音频系统。PulseAudio可以在用户空间运行，并且通常在ALSA之上运行。PulseAudio支持网络透明化、每应用程序的音量调节、空间事件声音、可以即时在设备之间切换音频流、策略判断以及其他许多高层次的操作。PulseAudio在Linux音频栈上加入了无缝播放glitch-free模式。在专业音频制作环境下PulseAudio并不是很有用。PulseAudio在Linux之间是可移植的。PulseAudio具有原生的API可供调用，并支持ALSA的安全子集Safe ALSA。PulseAudio支持基于LD_PRELOAD的OSS兼容。PulseAudio支持与JACK连接。
[APIReference]
OSS  
OSS LogoOpenSoundSystem是一种低层的PCM音频API，支持多种Unix，包括Linux。一开始的时候它是被作为标准的Linux音频系统并在目前的Linux核心中实现对API版本3支持，并称为OSS3。但是，OSS3已经被认为是过时的，并已经被ALSA完整地替代。OSS3的后继即OSS4已经可用，但在Linux上没有获得支持，包括标准核心以及众多的发行版。OSS API是非常低层的，是基于使用ioctl()函数的直接核心界面通信而实现的。因此，OSS3并不易用，而且并不能在非核心级别的音频系统比如象PulseAudio一类的声音服务器上实现，也不能在用户空间级别的音频驱动（比如蓝牙或火线音频）上实现。OSS3的时序模型完全无法正确地投射到软件音频服务器上，并在非PCI硬件诸如USB音频上也是存在问题的。还有就是，OSS不支持采样类型转换、remapping或重采样。这意味着对于支持OSS3的软件来说，它必须包括完整的转换器/remapper/resampler，以应对硬件本身不支持用户需要的音频参数时需要用软件搭救的情况。而现在常见的音频设备很多就是只支持采样率48KHz的S32LE（有符号32位低位在前）音频。如果OSS应用软件假设它总是能播放S16LE（有符号16位低位在前）采样率为44.1KHz的音频就会因此而失败。OSS3可移植到其他Unix类的操作系统，但不同的系统之间存在差异。还有就是，OSS不支持环绕声和其他现在的声音系统所提供的功能。因此，综上所述，不应该再使用OSS。
[ProgrammingGuide]
上面列出的所有音频系统和API一般在当前的各种发行版中均获得支持，除了libcanberra可能需要到开发中的发行版里面才有支持。

你想了解在什么情况下你必须使用某种特定的音频API吗？
GStreamer
GStreamer 适用于于非常高层的应用。比如你想播放音频或视频流，但不想理会任何的技术细节，诸如PCM播放控制或解码器调用等。
libcanberra
libcanberra对于那些需要在用户界面中加入声音反馈的应用是最适合的。另外，它也可用作播放简单的声音以达到向用户给出提示一类的用途。
JACK
JACK 最适合于专业音频制作，以及应用程序之间的互联。
Full ALSA
完整的 ALSA 界面最适合于那些在“plumbing layer”的软件，或你基于某些专业音频制作的目的而想使用某种特定硬件的特殊功能时。
Safe ALSA
safe ALSA 界面最适合于通过硬件或其他软件声音系统播放或录制PCM音频数据的软件。
Phonon and KNotify
Phonon 和 KNotify 只能在 KDE/Qt 应用程序中使用，并只适用于高层的媒体播放用途，比如简单的声音提示等。
SDL
SDL 最适合于在全屏幕游戏中使用。
PulseAudio
目前，PulseAudio API只应在那些想控制声音系统特定的功能（比如混音器）的应用程序，又或者，应用程序本身已具备PCM输出抽象层，想为PulseAudio增加额外的后端来保持一个最小化的音频栈。
OSS
OSS 不适合于在任何新的应用程序中使用。
你想了解多些有关safe ALSA 子集吗？
这里是一个“应”和“否”的列表，对应着ALSAAPI。如果你想你的软件能向后兼容，并且能在各种非硬件的后端或运行在用户空间的后端（比如蓝牙或火线音频）上运行，你应仔细阅读这个列表。其中一些建议也适用于使用完整ALSA API的软件开发者。如果你的软件不遵循这个列表，除非你有很好的理由，否则，可以认为你的软件是失败的。

“否”：

不要使用“异步句柄（async handlers）”。比如通过 snd_async_add_pcm_handler()及相关的函数。异步句柄Asynchronous handlers是通过POSIX信号实现的，但在函数库和插件中使用该技术是有疑问的。即使你不想把自己限制在安全ALSA子集中，也不建议使用该功能。 这篇文章解释了为何信号对于音频输入输出操作是恶魔。
不要自行分解ALSA配置文件，也不要使用任何ALSA配置功能函数，比如snd_config_xxx()那些。如果你需要枚举音频设备，则使用snd_device_name_hint()及相关的函数。它是唯一一个能同时支持枚举非硬件音频设备和用户空间音频设备的API。
不要分解/proc/asound/下面的任何文件。那些文件只包含有关于核心音频驱动的信息，用户空间插件并没有在那里列出。同时，核心设备的集合可能会与在用户空间所表现的不尽相同。比如，子设备会以其他名字（面向最终用户的名字，比如surround51）的方式被映射到用户空间。
不要依赖于ALSA给出的固定设备索引。设备是依赖于系统引导过程驱动程序的初始化顺序，因此并不是固定的。
不要使用snd_card_xxx() API。如果要进行枚举，可使用snd_device_name_hint()以及相关的函数。snd_card_xxx()已经过时了。它只能列出核心的硬件设备，不能列出用户空间设备。尤其是snd_card_load()已经完全淘汰。
不要硬编码设备名字符串，尤其是不要使用“hw:0”或“plughw:0”或“dmix”。这些设备没有定义通道映射，而且本身是直接映射到原始核心设备的。建议只使用“default”作为设备名字符串。如果需要特定的通道映射，对于立体声来说应使用“front”，4声道立体声应使用“surround40”，以及“surround41”、“surround51”等。可惜的是在目前，ALSA并没有为非核心设备的通道映射定义标准名称。也即只有“default”是对于单声道或立体声音频流的唯一安全选择。你还应该在你的设备字符串前加上“plug:”前缀以使ALSA能透明地重新格式/映射/重采样你的PCM音频流，以便应对硬件或后端并不能原生地支持你的采样参数时的情况。
不要假设任何特定的采样样本类别都能被支持，除了以下这些：U8, S16_LE, S16_BE, S32_LE, S32_BE,FLOAT_LE, FLOAT_BE, MU_LAW, A_LAW。
不要使用snd_pcm_avail_update()来进行同步控制用途。当前这个函数只应用来查询还剩下多少字节需要读或者写。不要使用 snd_pcm_delay()来查询播放缓冲区的填充情况。它只能用来作同步用途。务必完全理解这两个函数的区别，并且注意这两个函数的返回值并不需要有任何直接联系。
不要假设混音器总是能理解dB信息。
不要假设所有的设备都支持MMAP类型的缓冲区访问。
不要假设播放缓冲区内的硬件指针就是DAC内部的实际播放位置。因为还存在着外部延时。
不要试图用自己的代码来从ALSA错误状态中恢复。一个例子是缓冲区欠载。应使用snd_pcm_recover()来实现。
不要修改缓冲/时段衡量单位，除非需要实现特定的时延要求。程序应该有防护性，能正确处理当后端不能支持你的缓冲时段要求的情况。还要注意，在大多数的情况下播放缓冲区的缓冲时段设置只能间接地影响整体的时延。比如，把缓冲区的大小设置为某个固定数值反而有可能导致更高的实际时延。
不要假设snd_pcm_rewind()是一定可用的，或者能在某种程度上可用的。
不要假设一个PCM流何时能收到新的数据是严格依赖于采样频率和缓冲区设置，从而形成一个所谓的“平均数据流量”的概念。必须在设备要求提供数据时总是能有新数据写入到设备，捕获数据也是一样。
不要使用“简单的”界面snd_spcm_xxx()。
不要使用任何已经标记为“过时”的函数。
不要使用定时器、midi、rawmidi以及硬件依赖的任何子系统。定时器和midi不推荐使用的原因是他们不受libasound的插件支持。
“应”：

使用snd_device_name_hint()来枚举音频设备。
使用snd_smixer_xxx()取代原始的snd_ctl_xxx()。
如果需要同步，使用snd_pcm_delay()。
如果需要检查缓冲区的播放/捕获填充情况，使用snd_pcm_update_avail()。
使用snd_pcm_recover()来从任何ALSA函数库返回的错误状态中恢复。
如果可能，使用设备所能支持的最大尺寸的缓冲区。这样可以最大限度地节约能源消耗和防止意外崩溃。如果需要快速地对用户输入进行响应的话，使用snd_pcm_rewind()
FAQ
ESD 和 NAS呢？
ESD 和 NAS 已经过时，无论是作为API抑或作为守护进程来使用。不要再基于他们来做开发。
ALSA 并不是可移植的！
这并不正确！实际上，用户空间函数库是相对可移植的，它甚至包括了一个专门支持OSS声音设备的后端。因而，没有理由不能在其他类Unix系统上使用ALSA库。
可移植性对我来说很重要！我能做些什么？
很不幸的是，截至目前为止，我没有能推荐的真正全面可移植的（比如，移植到WIN32）PCMAPI。虽然上面介绍的各种系统都或多或少地在类Unix系统之间可移植，但这并不表示这些系统在任何平台上都有可用的受支持的后端。如果你关心能否移植到Win32平台和MacOS，你只能在上面的建议之外寻找一个解决方案，或针对以上的方案提交你的可移植性或后端支持的修补。以上介绍的系统除了OSS之外没有一个是与Linux或类Unix系统的核心绑定的。
对PortAudio你的看法是？
我不认为对于类Unix系统来说，PortAudio是一个好的API。因此我不建议使用，但这是你的选择。
为何你这么讨厌OSS4？
我并不讨厌任何人或任何事情。我只是不认为OSS4是一个严肃的选择，特别是在Linux上。而且，对于Linux来说，由于有了ALSA，OSS已经完全是一个冗余。
我知道有些新的项目具备非常好的音频/媒体抽象层设计！
这并不足够。我只列出已知是有足够的相关性和维护得很好的那些。
后记
当然，以上这些建议都是非常基础的，只能作为一个方向性的指引。对于每一个具体的用例，实际上都有不同的细节上的需求，但我这里没有给予考虑。你应该考虑对于你的应用软件，我这些建议究竟有多少是实际适用的。

这个汇总只包括被认为是稳定的和在此时相对通用的那些系统。在未来，我希望能出现相对safeALAS子集来说更适合以及更具备可移植性的替代品。

（作者的其他一些感语忽略不译了）

译者语
评论指出，作者还漏了一些其他的音频API没有描述。尤其是：OpenAL。OpenAl是唯一一种支持多扬声器配置的音频函数库，并在那些需要使用到三维定位音效的游戏中被大量使用。比如Doom3这样的游戏都是使用OpenAL的。对于平常的多媒体用途，可能通过OpenAL来播放DVD上的多声道音频是一种比较常见的用途。OpenAL还有一个特点是，它已经是一个可以横跨Windows/Linux/OSX三大平台的系统，而且并不是只能作为游戏的后端使用。

另外，作者这篇发表在个人Blog上的文章引起了KDE粉丝的不满，认为作者有偏颇。而作者表示虽然自己是GNOME基础成员之一，但以上的一切只是个人看法。在滔滔的评论中，有一段是值得在这里列出的：

如果你的应用软件已经使用了GTK+，可以通过GST或libcanberra来加入声音功能；
如果你的应用软件使用的是Qt或Qt/KDE函数库，那么Phonon是一个比较自然的选择；
如果你打算写一个全新的应用软件，而且还没有与任何工具平台绑定，可以考虑Phonon所带来的后端抽象化的优点，与之对比的是直接使用GST或Xine所得到的可以进行细节控制的功能。



# 原文出处：

* http://www.cnblogs.com/qyddbear/articles/2691142.html
* http://www.techradar.com/news/audio/linux-audio-explained-685419/1
